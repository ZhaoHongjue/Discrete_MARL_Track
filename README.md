# Discrete_MARL_Track

**概述**：用网格表示地图，每个地图可以是机器人、障碍物、被捕物，或者什么都没有。需要在这个地图下完成追踪围捕。机器人和被捕物都可以向周围的方格运动，并且存在一定的运动学约束（例如每次最大移动方格数目）

整个过程可以分为以下三个阶段来实现：

1. 单智能体的导航
2. 多智能体的协同导航
3. 多智能体的协同追踪围捕

计划采用算法：Deep RL中的DQN算法，以及策略梯度、执行者/评论者等算法。

## 单智能体导航

在有障碍物的环境中单智能体绕过障碍物，到达目标点。

1. env.py：环境设置
   - 初始化`__init__`：
     1. 初始化生成地图
     2. 生成agents的列表
     3. 放置Agent和障碍物
   - 放置机器人`AddAgents`：根据指定的机器人个数，随机生成机器人的初始位置和目标点
   - 放置障碍物`AddBlocks`：在地图中放置障碍物
   - 机器人观测`Agents_Observe`：让环境中的每个机器人进行观测，观测以自身为中心的5*5的矩阵信息，之后展平，并把目标点在局部坐标系的位置加入其中
   - `step`：到交互一轮，返回观测、奖励和结束标志
   - `reset`：重置环境
   - `render`：绘制环境
2. Agent.py：单个机器人的控制

    - 初始化`__init__`：参数包括：
      1. 自身位置参数：机器人在世界坐标系下的初始位置，当前在世界坐标系下的位置
      2. 目标点参数：目标点在世界坐标系下的位置，目标点在机器人自己坐标系下的位置
      3. 任务类参数：到达目标点、发生碰撞等
      4. 指定机器人是我方机器人还是敌方机器人
    - 重置`reset`：使机器人回到初始坐标，更新局部规划中目标点的位置
    - 设定目标点`set_goal`：设定机器人的目标点
    - 选择动作`set_action`：向周围八个方向移动，生成长度为2的向量，直接与机器人的位置进行加减
    - 计算奖励`compute_reward`：返回相应的奖励

