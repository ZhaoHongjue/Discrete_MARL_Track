# Discrete_MARL_Track

**概述**：用网格表示地图，每个地图可以是机器人、障碍物、被捕物，或者什么都没有。需要在这个地图下完成追踪围捕。机器人和被捕物都可以向周围的方格运动，并且存在一定的运动学约束（例如每次最大移动方格数目）

整个过程可以分为以下三个阶段来实现：

1. 单智能体的导航
2. 多智能体的协同导航
3. 多智能体的协同追踪围捕

计划采用算法：Deep RL中的DQN算法，以及策略梯度、执行者/评论者等算法。

## 单智能体导航

在有障碍物的环境中单智能体绕过障碍物，到达目标点。

1. env.py：环境设置
   - 初始化`__init__`：
     1. 地图初始构建：指定范围
     2. 放置障碍物
     3. 放置Agent
   - `step`：到交互一轮，返回观测、奖励和结束标志
   - `reset`：重置所有
2. Agent.py：单个机器人的控制

    - 初始化`__init__`：参数包括：
      1. 自身位置参数：机器人在世界坐标系下的初始位置，当前在世界坐标系下的位置
      2. 目标点参数：目标点在世界坐标系下的位置，目标点在机器人自己坐标系下的位置
      3. 观测：以机器人自己为中心的5*5方格中的各种信息（障碍物、目标点）
      4. 任务类参数：到达目标点、发生碰撞等
      5. 指定机器人是我方机器人还是敌方机器人
    - 重置`reset`：使机器人回到初始坐标，更新初始观测和目标位置并返回
    - 设定目标点`set_goal`：设定机器人的目标点
    - 选择动作`set_action`：向周围八个方向移动，生成长度为2的向量，直接与机器人的位置进行加减
    - 获取状态`get_state`：获取周围环境信息
    - 计算奖励`compute_reward`：返回相应的奖励

